# ë³´í—˜ì•½ê´€ ê¸°ë°˜ Agentic AI ì‹œìŠ¤í…œ - ê¸°ìˆ  ì—°êµ¬ ë³´ê³ ì„œ v2.0

**ì‘ì„±ì¼**: 2025ë…„ 10ì›” 27ì¼  
**í”„ë¡œì íŠ¸ëª…**: ISPL (Insurance Policy) - Agentic AI System  
**ë²„ì „**: v2.0 (êµ¬í˜„ ì™„ë£Œ ë²„ì „)

---

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” ì‹¤ì œ êµ¬í˜„ëœ ISPL ì‹œìŠ¤í…œì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ëœ ê¸°ìˆ  ì—°êµ¬ ë³´ê³ ì„œì…ë‹ˆë‹¤. ì´ˆê¸° ì—°êµ¬ ë³´ê³ ì„œ(v1.0)ì™€ ë‹¬ë¦¬, ì‹¤ì œ êµ¬í˜„ ìƒíƒœë¥¼ ë°˜ì˜í•˜ì—¬ ì•„í‚¤í…ì²˜, ê¸°ìˆ  ìŠ¤íƒ, êµ¬í˜„ íŒ¨í„´, ì„±ëŠ¥ ìµœì í™” ì „ëµì„ ìƒì„¸íˆ ê¸°ìˆ í•©ë‹ˆë‹¤.

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©ì 
- ìƒì„±í˜• AIë¥¼ í™œìš©í•œ ë³´í—˜ì•½ê´€ ì „ì²˜ë¦¬, ì„ë² ë”©, ë²¡í„°DB ì €ì¥
- ìì—°ì–´ ì§ˆì˜ì— ëŒ€í•œ ì •í™•í•œ ì•½ê´€ ê²€ìƒ‰ ë° ë‹µë³€ ì œê³µ
- í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ë° ì‹ ë¢°ë„ ê¸°ë°˜ ë‹µë³€ ìƒì„±
- íŒŒì¼ ì—…ë¡œë“œ ë° ì•½ê´€ í†µí•© ê´€ë¦¬ ê¸°ëŠ¥ ì œê³µ

### 1.2 í”„ë¡œì íŠ¸ ë²”ìœ„
- **ì•½ê´€ ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬**: PDF â†’ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ â†’ Markdown â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ë²¡í„°DB ì €ì¥
- **ì•½ê´€ ê²€ìƒ‰**: ì‚¬ìš©ì ì§ˆì˜ ì „ì²˜ë¦¬ â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„°+í‚¤ì›Œë“œ) â†’ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” â†’ LLM ë‹µë³€ ìƒì„± â†’ ë‹µë³€ ê²€ì¦
- **ì•½ê´€ ê´€ë¦¬**: ì•½ê´€ ëª©ë¡ ì¡°íšŒ, ì›ë³¸/Markdown íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì‚­ì œ, PDF ì¡°íšŒ
- **ëŒ€í™” ì´ë ¥**: ì„¸ì…˜ë³„ ëŒ€í™” ì´ë ¥ ì €ì¥ ë° ì¡°íšŒ

### 1.3 ì£¼ìš” íŠ¹ì§•
- **LangGraph ê¸°ë°˜ Multi-Agent ì‹œìŠ¤í…œ**: 7ê°œì˜ ì „ë¬¸í™”ëœ ì—ì´ì „íŠ¸ë¡œ êµ¬ì„±
- **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ë²¡í„° ê²€ìƒ‰(ì˜ë¯¸ ê¸°ë°˜) + í‚¤ì›Œë“œ ê²€ìƒ‰(ì •í™•í•œ ë§¤ì¹­) ìœµí•©
- **ë‹µë³€ ê²€ì¦ ì‹œìŠ¤í…œ**: 4ë‹¨ê³„ ê²€ì¦(í• ë£¨ì‹œë„¤ì´ì…˜, ì»¨í…ìŠ¤íŠ¸, ì¡°í•­, í˜•ì‹)ìœ¼ë¡œ ì‹ ë¢°ë„ ë³´ì¥
- **ì²­í¬ í™•ì¥ ë©”ì»¤ë‹ˆì¦˜**: ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ ì£¼ë³€ ì²­í¬ í™•ì¥
- **Redis ìºì‹±**: ì„ë² ë”© ê²°ê³¼ ìºì‹±ìœ¼ë¡œ API ë¹„ìš© ì ˆê° ë° ì„±ëŠ¥ í–¥ìƒ

### 1.4 ì œì•½ ì¡°ê±´
- PoC ë‹¨ê³„ë¡œ ëŒ€ê·œëª¨ íŠ¸ë˜í”½ì€ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
- ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìš°ì„ 
- PDF íŒŒì¼ë§Œ ì§€ì› (ë‹¤ë¥¸ í˜•ì‹ì€ ë¯¸ì§€ì›)

---

## 2. ê¸°ìˆ  ìŠ¤íƒ

### 2.1 Backend
- **Framework**: FastAPI 0.115.0 (Python)
- **AI Framework**: 
  - LangGraph 0.3.27+ (Multi-Agent ì›Œí¬í”Œë¡œìš°)
  - LangChain 0.3.7 (LLM í†µí•©)
  - LangChain-OpenAI 0.2.8 (OpenAI ì—°ë™)
- **Database**: 
  - PostgreSQL 17.6 (ë©”ì¸ DB)
  - pgvector 0.3.6 (ë²¡í„° í™•ì¥)
  - asyncpg 0.30.0 (ë¹„ë™ê¸° ë“œë¼ì´ë²„)
  - SQLAlchemy 2.0.35 (ORM)
- **Cache**: Redis 5.2.1 (ì„ë² ë”© ìºì‹œ)
- **LLM**: 
  - OpenAI GPT-4o (ë‹µë³€ ìƒì„± ë° Vision API)
  - Temperature: 0.1 (ì¼ê´€ì„± ìˆëŠ” ë‹µë³€)
- **Embedding**: OpenAI text-embedding-3-large (1536 ì°¨ì›)
- **PDF ì²˜ë¦¬**: 
  - PyMuPDF 1.24.14 (PDF íŒŒì‹±)
  - pymupdf4llm 0.0.17 (Markdown ë³€í™˜)
  - pdf2image 1.17.0 (ì´ë¯¸ì§€ ë³€í™˜)
  - GPT-4o Vision (ì´ë¯¸ì§€ ê¸°ë°˜ ì¶”ì¶œ)
- **ì´ë¯¸ì§€ ì²˜ë¦¬**:
  - OpenCV 4.10.0 (ì „ì²˜ë¦¬)
  - Pillow 11.0.0 (ì´ë¯¸ì§€ ì¡°ì‘)
- **ìœ í‹¸ë¦¬í‹°**:
  - tiktoken 0.8.0 (í† í° ì¹´ìš´íŒ…)
  - kiwipiepy 0.21.0 (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„)
  - tenacity 9.0.0 (ì¬ì‹œë„ ë¡œì§)

### 2.2 Frontend
- **Framework**: Next.js 15.0.3 (App Router)
- **UI Library**: React 18.3.1
- **Language**: TypeScript 5.6.3
- **Styling**: Tailwind CSS 3.4.14
- **Markdown**: 
  - react-markdown 9.0.1
  - remark-gfm 4.0.0 (GitHub Flavored Markdown)

### 2.3 ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
ì‚¬ìš©ì (ë¸Œë¼ìš°ì €)
    â†“
Next.js Frontend (í¬íŠ¸ 3000)
    â†“ HTTP/REST
FastAPI Backend (í¬íŠ¸ 8000)
    â†“
LangGraph Multi-Agent System
    â”œâ”€ Router Agent (ë¼ìš°íŒ…)
    â”œâ”€ Search Agent (ê²€ìƒ‰)
    â”œâ”€ Context Judgement Agent (ì»¨í…ìŠ¤íŠ¸ íŒë‹¨)
    â”œâ”€ Chunk Expansion Agent (ì²­í¬ í™•ì¥)
    â”œâ”€ Answer Agent (ë‹µë³€ ìƒì„±)
    â”œâ”€ Processing Agent (PDF ì „ì²˜ë¦¬)
    â””â”€ Management Agent (ì•½ê´€ ê´€ë¦¬)
    â†“
PostgreSQL + pgvector + Redis
    â†“
OpenAI API (GPT-4o, Embedding)
```

---

## 3. LangGraph Multi-Agent ì•„í‚¤í…ì²˜

### 3.1 Agent êµ¬ì„± (7ê°œ)

#### 1. Router Agent (ë¼ìš°í„°)
- **ì—­í• **: ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ë° ì ì ˆí•œ Agentë¡œ ë¼ìš°íŒ…
- **ì…ë ¥**: ì‚¬ìš©ì ì§ˆì˜
- **ì¶œë ¥**: ì˜ë„ ë¶„ë¥˜ ë° Command ê°ì²´
- **ë¼ìš°íŒ… ê·œì¹™**:
  - `search`: Search Agentë¡œ ì „ë‹¬ (ê²€ìƒ‰ ë° ë‹µë³€)
  - `upload`: Processing Agentë¡œ ì „ë‹¬ (PDF ì²˜ë¦¬)
  - `manage`: Management Agentë¡œ ì „ë‹¬ (ì•½ê´€ ê´€ë¦¬)

#### 2. Search Agent (ê²€ìƒ‰)
- **ì—­í• **: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
- **ê²€ìƒ‰ ë°©ì‹**:
  - ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜, Cosine Similarity)
  - í‚¤ì›Œë“œ ê²€ìƒ‰ (Full-Text Search)
  - RRF(Reciprocal Rank Fusion) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²°ê³¼ ìœµí•©
- **ê²€ìƒ‰ ë²”ìœ„**: ìµœëŒ€ 20,000 í† í° (GPT-4o ì»¨í…ìŠ¤íŠ¸)
- **ìœ ì‚¬ë„ ì„ê³„ê°’**: 0.65 (Cosine Similarity)
- **ì¶œë ¥**: ê²€ìƒ‰ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸

#### 3. Context Judgement Agent (ì»¨í…ìŠ¤íŠ¸ íŒë‹¨)
- **ì—­í• **: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µí•˜ê¸°ì— ì¶©ë¶„í•œì§€ íŒë‹¨
- **íŒë‹¨ ê¸°ì¤€**:
  - ì²­í¬ ê°œìˆ˜ (ìµœì†Œ 3ê°œ ì´ìƒ)
  - ë‚´ìš© ì¼ì¹˜ë„
  - ì¡°í•­ ë²ˆí˜¸ ì¡´ì¬ ì—¬ë¶€
- **ì¶œë ¥**: 
  - `sufficient=True` â†’ Answer Agentë¡œ ë¼ìš°íŒ…
  - `sufficient=False` â†’ Chunk Expansion Agentë¡œ ë¼ìš°íŒ…

#### 4. Chunk Expansion Agent (ì²­í¬ í™•ì¥)
- **ì—­í• **: ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ ì£¼ë³€ ì²­í¬ë¥¼ í™•ì¥í•˜ì—¬ ì¶”ê°€
- **í™•ì¥ ì „ëµ**:
  - ê°™ì€ ë¬¸ì„œì˜ ì•ë’¤ ì²­í¬ í™•ì¥ (Â±2 ì¸ë±ìŠ¤)
  - ê°™ì€ í˜ì´ì§€ì˜ ì²­í¬ í™•ì¥
  - ìµœëŒ€ 3íšŒê¹Œì§€ í™•ì¥ ì‹œë„
- **ì¶œë ¥**: í™•ì¥ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
- **ì¬íŒë‹¨**: Context Judgement Agentë¡œ ë‹¤ì‹œ ì „ë‹¬

#### 5. Answer Agent (ë‹µë³€ ìƒì„±)
- **ì—­í• **: LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ë° ê²€ì¦
- **ëª¨ë¸**: GPT-4o (temperature: 0.1)
- **í”„ë¡¬í”„íŠ¸**:
  - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ë³´í—˜ì•½ê´€ ì „ë¬¸ ìƒë‹´ì‚¬ ì—­í• 
  - ì»¨í…ìŠ¤íŠ¸: ê²€ìƒ‰ëœ ì²­í¬ë“¤
  - ì‚¬ìš©ì ì§ˆì˜
- **ì¶œë ¥ í˜•ì‹**:
  ```
  ğŸ“Œ ë‹µë³€
  [ë‹µë³€ ë‚´ìš©] [ì°¸ì¡° 1]
  
  ğŸ“‹ ê´€ë ¨ ì•½ê´€
  [ì°¸ì¡° 1] ë¬¸ì„œëª… | í˜ì´ì§€ | ì¡°í•­
  [ë‚´ìš©]
  ```
- **ê²€ì¦**: AnswerValidator ì„œë¹„ìŠ¤ í˜¸ì¶œ

#### 6. Processing Agent (PDF ì „ì²˜ë¦¬)
- **ì—­í• **: PDF ì—…ë¡œë“œ ë° í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬
- **ì²˜ë¦¬ ë°©ì‹**:
  - `pymupdf`: PyMuPDF4LLMìœ¼ë¡œ ë¹ ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
  - `vision`: GPT-4o Visionìœ¼ë¡œ ì´ë¯¸ì§€ ê¸°ë°˜ ì¶”ì¶œ
  - `both`: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (ê¸°ë³¸ê°’)
- **ì²˜ë¦¬ ë‹¨ê³„**:
  1. PDF ì—…ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
  2. Path 1 (PyMuPDF) ì‹¤í–‰
  3. Path 2 (Vision API) ì‹¤í–‰
  4. ê²°ê³¼ ë³‘í•© (ìœ ì‚¬ë„ ê¸°ë°˜)
  5. í’ˆì§ˆ ê²€ì¦
  6. ì²­í‚¹ (1000 í† í°, overlap 100)
  7. ì„ë² ë”© ìƒì„± (ìºì‹± í™œìš©)
  8. ë²¡í„°DB ì €ì¥
- **ì¶œë ¥**: ì²˜ë¦¬ ê²°ê³¼ ë° í’ˆì§ˆ ì ìˆ˜

#### 7. Management Agent (ê´€ë¦¬)
- **ì—­í• **: ì•½ê´€ CRUD ì‘ì—…
- **ê¸°ëŠ¥**:
  - ì•½ê´€ ëª©ë¡ ì¡°íšŒ (í˜ì´ì§•, í•„í„°ë§)
  - ì•½ê´€ ì‚­ì œ (ë¬¸ì„œ + ì²­í¬ + íŒŒì¼)
  - ì›ë³¸/Markdown íŒŒì¼ ë‹¤ìš´ë¡œë“œ
  - ì•½ê´€ ìƒì„¸ ì •ë³´ ì¡°íšŒ

### 3.2 LangGraph StateGraph êµ¬ì¡°

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

builder = StateGraph(ISPLState)

# ë…¸ë“œ ì¶”ê°€
builder.add_node("router", router_node)
builder.add_node("search_agent", search_node)
builder.add_node("context_judgement_agent", context_judgement_node)
builder.add_node("chunk_expansion_agent", chunk_expansion_node)
builder.add_node("answer_agent", answer_node)
builder.add_node("processing_agent", processing_node)
builder.add_node("management_agent", management_node)

# ì—£ì§€ ì •ì˜
builder.add_edge(START, "router")
builder.add_edge("search_agent", "context_judgement_agent")

# ì¡°ê±´ë¶€ ë¼ìš°íŒ…
def route_after_judgement(state: ISPLState) -> str:
    if state.get("context_sufficient", True):
        return "answer_agent"
    else:
        return "chunk_expansion_agent"

builder.add_conditional_edges(
    "context_judgement_agent",
    route_after_judgement,
    {
        "answer_agent": "answer_agent",
        "chunk_expansion_agent": "chunk_expansion_agent"
    }
)

builder.add_edge("chunk_expansion_agent", "context_judgement_agent")
builder.add_edge("answer_agent", END)
builder.add_edge("processing_agent", END)
builder.add_edge("management_agent", END)

# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸íŠ¸
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)
```

### 3.3 State ê´€ë¦¬

```python
from langgraph.graph import MessagesState
from typing import TypedDict, List, Optional

class ISPLState(MessagesState):
    """ISPL LangGraph ìƒíƒœ"""
    query: str  # ì‚¬ìš©ì ì§ˆì˜
    next_agent: str  # ë‹¤ìŒ ì—ì´ì „íŠ¸
    task_type: str  # ì‘ì—… ìœ í˜• (search/upload/manage)
    task_results: dict  # ì‘ì—… ê²°ê³¼
    search_results: List[dict]  # ê²€ìƒ‰ ê²°ê³¼
    final_answer: str  # ìµœì¢… ë‹µë³€
    error: Optional[str]  # ì˜¤ë¥˜ ë©”ì‹œì§€
    context_sufficient: Optional[bool]  # ì»¨í…ìŠ¤íŠ¸ ì¶©ë¶„ ì—¬ë¶€
    expanded_chunks: List[dict]  # í™•ì¥ëœ ì²­í¬
    expansion_count: int  # í™•ì¥ íšŸìˆ˜
    chunks_to_expand: List[int]  # í™•ì¥í•  ì²­í¬ ID
```

---

## 4. FastAPI + PostgreSQL + pgvector í†µí•©

### 4.1 Database ì—°ê²° íŒ¨í„´

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from typing import AsyncGenerator

# ë¹„ë™ê¸° Engine ìƒì„±
engine = create_async_engine(
    f"postgresql+asyncpg://{user}:{password}@{host}:{port}/{database}",
    echo=False,  # SQL ë¡œê¹…
    pool_size=10,  # ì—°ê²° í’€ í¬ê¸°
    max_overflow=20  # ìµœëŒ€ ì˜¤ë²„í”Œë¡œìš°
)

# Session Factory
async_session_maker = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=False
)

# Dependency Injection
async def get_session() -> AsyncGenerator[AsyncSession, None]:
    async with async_session_maker() as session:
        try:
            yield session
        finally:
            await session.close()
```

### 4.2 pgvector ë²¡í„° ê²€ìƒ‰

```python
from sqlalchemy import text

async def vector_search(
    session: AsyncSession,
    query_embedding: list[float],
    limit: int = 10,
    similarity_threshold: float = 0.65,
    document_ids: Optional[List[int]] = None
) -> List[VectorSearchResult]:
    """
    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    
    HNSW ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
    """
    query = text("""
        SELECT 
            dc.id,
            dc.document_id,
            dc.chunk_index,
            dc.content,
            dc.page_number,
            dc.pdf_page_number,
            dc.clause_number,
            dc.section_title,
            dc.metadata,
            d.filename,
            d.original_filename,
            d.document_type,
            1 - (dc.embedding <=> :query_embedding) as similarity
        FROM document_chunks dc
        JOIN documents d ON dc.document_id = d.id
        WHERE 1 - (dc.embedding <=> :query_embedding) > :threshold
            AND d.status = 'active'
            AND (:document_ids::int[] IS NULL OR dc.document_id = ANY(:document_ids))
        ORDER BY dc.embedding <=> :query_embedding
        LIMIT :limit
    """)
    
    result = await session.execute(query, {
        "query_embedding": query_embedding,
        "threshold": similarity_threshold,
        "limit": limit,
        "document_ids": document_ids
    })
    
    return [VectorSearchResult(**row._mapping) for row in result.fetchall()]
```

### 4.3 Full-Text í‚¤ì›Œë“œ ê²€ìƒ‰

```python
async def keyword_search(
    session: AsyncSession,
    query: str,
    limit: int = 10
) -> List[dict]:
    """
    í‚¤ì›Œë“œ ê²€ìƒ‰ (PostgreSQL Full-Text Search)
    
    ì‚¬ìš©: ì¡°í•­ ë²ˆí˜¸, ì „ë¬¸ìš©ì–´ ë“± ì •í™•í•œ ë§¤ì¹­
    """
    # í‚¤ì›Œë“œ ì¶”ì¶œ (ì¡°ì‚¬ ì œê±°)
    keywords = extract_keywords(query)
    if not keywords:
        return []
    
    # tsquery ìƒì„±
    tsquery = ' & '.join(keywords)
    
    sql = text("""
        SELECT 
            dc.id,
            dc.document_id,
            dc.content,
            dc.page_number,
            dc.clause_number,
            d.filename,
            ts_rank(to_tsvector('simple', dc.content), 
                    to_tsquery('simple', :tsquery)) as rank
        FROM document_chunks dc
        JOIN documents d ON dc.document_id = d.id
        WHERE to_tsvector('simple', dc.content) @@ to_tsquery('simple', :tsquery)
            AND d.status = 'active'
        ORDER BY rank DESC
        LIMIT :limit
    """)
    
    result = await session.execute(sql, {
        "tsquery": tsquery,
        "limit": limit
    })
    
    return [dict(row._mapping) for row in result.fetchall()]
```

### 4.4 HNSW ì¸ë±ìŠ¤ ìµœì í™”

```sql
-- pgvector extension í™œì„±í™”
CREATE EXTENSION IF NOT EXISTS vector;

-- HNSW ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_chunks_embedding ON document_chunks 
USING hnsw (embedding vector_cosine_ops) 
WITH (
    m = 32,              -- ê·¸ë˜í”„ ì—°ê²° ìˆ˜ (ê¸°ë³¸ê°’: 16, ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
    ef_construction = 200 -- ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„ (ê¸°ë³¸ê°’: 64)
);

-- ì¸ë±ìŠ¤ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,  -- ì¸ë±ìŠ¤ ìŠ¤ìº” íšŸìˆ˜
    idx_tup_read,  -- ì½ì€ í–‰ ìˆ˜
    idx_tup_fetch  -- ê°€ì ¸ì˜¨ í–‰ ìˆ˜
FROM pg_stat_user_indexes
WHERE indexname = 'idx_chunks_embedding';
```

---

## 5. PDF í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 5.1 ì „ì²´ ì²˜ë¦¬ íë¦„

```
PDF ì…ë ¥
   â”œâ”€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (í˜ì´ì§€ ìˆ˜, ì œëª© ë“±)
   â”‚
   â”œâ”€ Path 1: PyMuPDF4LLM
   â”‚   â”œâ”€ í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œ
   â”‚   â”œâ”€ Markdown ë³€í™˜
   â”‚   â””â”€ êµ¬ì¡°í™” (í‘œ, ì´ë¯¸ì§€ ê°ì§€)
   â”‚
   â”œâ”€ Path 2: GPT-4o Vision (ì„ íƒì )
   â”‚   â”œâ”€ PDF â†’ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜ (DPI 300)
   â”‚   â”œâ”€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ê·¸ë ˆì´ìŠ¤ì¼€ì¼, ë…¸ì´ì¦ˆ ì œê±°)
   â”‚   â”œâ”€ GPT-4o Vision API í˜¸ì¶œ
   â”‚   â””â”€ Markdown ê²°ê³¼ ìƒì„±
   â”‚
   â”œâ”€ ê²°ê³¼ ë³‘í•© (ìœ ì‚¬ë„ ê¸°ë°˜)
   â”‚   â”œâ”€ SequenceMatcherë¡œ ìœ ì‚¬ë„ ê³„ì‚°
   â”‚   â”œâ”€ ì¤‘ë³µ ì œê±°
   â”‚   â””â”€ ìµœì  ê²°ê³¼ ì„ íƒ
   â”‚
   â”œâ”€ í’ˆì§ˆ ê²€ì¦
   â”‚   â”œâ”€ ì™„ì „ì„± ì ìˆ˜
   â”‚   â”œâ”€ ì¼ê´€ì„± ê²€ì‚¬
   â”‚   â””â”€ ì‹ ë¢°ë„ í‰ê°€
   â”‚
   â”œâ”€ ì²­í‚¹ (Fixed-size Chunking)
   â”‚   â”œâ”€ Chunk Size: 1000 í† í°
   â”‚   â”œâ”€ Overlap: 100 í† í°
   â”‚   â”œâ”€ íŠ¹ìˆ˜ ì²˜ë¦¬: í‘œ(í†µì§¸ë¡œ), ì´ë¯¸ì§€(ì„¤ëª…+ì£¼ë³€)
   â”‚   â””â”€ ë©”íƒ€ë°ì´í„° ë¶€ì—¬ (í˜ì´ì§€, ì„¹ì…˜, ì¡°í•­)
   â”‚
   â”œâ”€ ì„ë² ë”© ìƒì„±
   â”‚   â”œâ”€ OpenAI text-embedding-3-large
   â”‚   â”œâ”€ ì°¨ì›: 1536
   â”‚   â”œâ”€ ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
   â”‚   â””â”€ Redis ìºì‹±
   â”‚
   â””â”€ ë²¡í„° DB ì €ì¥
       â”œâ”€ document_chunks í…Œì´ë¸”
       â”œâ”€ HNSW ì¸ë±ìŠ¤ ìë™ ì—…ë°ì´íŠ¸
       â””â”€ ì²˜ë¦¬ ë¡œê·¸ ê¸°ë¡
```

### 5.2 Path 1: PyMuPDF4LLM ì¶”ì¶œ

```python
import pymupdf4llm

class PyMuPDFExtractor:
    """PyMuPDF4LLMì„ ì‚¬ìš©í•œ PDF â†’ Markdown ë³€í™˜"""
    
    def extract(self, pdf_path: str) -> dict:
        """
        PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  Markdownìœ¼ë¡œ ë³€í™˜
        
        Returns:
            {
                'markdown': str,  # ë³€í™˜ëœ Markdown
                'pages': int,     # ì´ í˜ì´ì§€ ìˆ˜
                'quality_score': float,  # í’ˆì§ˆ ì ìˆ˜
                'tables': List[dict],    # ê°ì§€ëœ í‘œ
                'images': List[dict]     # ê°ì§€ëœ ì´ë¯¸ì§€
            }
        """
        # PyMuPDF4LLMë¡œ Markdown ë³€í™˜
        md_text = pymupdf4llm.to_markdown(
            pdf_path,
            page_chunks=True,  # í˜ì´ì§€ë³„ ì²­í‚¹
            write_images=True,  # ì´ë¯¸ì§€ ì¶”ì¶œ
            image_path="uploads/images",  # ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
            dpi=300  # ì´ë¯¸ì§€ í•´ìƒë„
        )
        
        # í‘œ ê°ì§€ (Markdown í…Œì´ë¸” êµ¬ë¬¸)
        tables = self._detect_tables(md_text)
        
        # ì´ë¯¸ì§€ ê°ì§€ (![](path) êµ¬ë¬¸)
        images = self._detect_images(md_text)
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._calculate_quality(md_text, tables, images)
        
        return {
            'markdown': md_text,
            'pages': self._count_pages(pdf_path),
            'quality_score': quality_score,
            'tables': tables,
            'images': images
        }
```

### 5.3 Path 2: GPT-4o Vision ì¶”ì¶œ

```python
from pdf2image import convert_from_path
from openai import AsyncOpenAI
import base64

class VisionExtractor:
    """GPT-4o Visionì„ ì‚¬ìš©í•œ PDF â†’ Markdown ë³€í™˜"""
    
    async def extract_async(self, pdf_path: str) -> dict:
        """
        PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ GPT-4o Visionìœ¼ë¡œ ë¶„ì„
        
        Returns:
            {
                'markdown': str,
                'pages': int,
                'quality_score': float,
                'api_calls': int,  # API í˜¸ì¶œ íšŸìˆ˜
                'total_cost': float  # ì¶”ì • ë¹„ìš©
            }
        """
        # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ (DPI 300)
        images = convert_from_path(pdf_path, dpi=300, fmt='png')
        
        # ê° í˜ì´ì§€ë¥¼ Vision APIë¡œ ì²˜ë¦¬
        page_results = []
        for idx, img in enumerate(images):
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_img = self._preprocess_image(img)
            
            # Base64 ì¸ì½”ë”©
            img_base64 = self._encode_image(processed_img)
            
            # Vision API í˜¸ì¶œ
            markdown = await self._call_vision_api(img_base64, idx + 1)
            page_results.append(markdown)
        
        # í˜ì´ì§€ ë³‘í•©
        full_markdown = "\n\n---\n\n".join(page_results)
        
        return {
            'markdown': full_markdown,
            'pages': len(images),
            'quality_score': self._estimate_quality(full_markdown),
            'api_calls': len(images),
            'total_cost': self._estimate_cost(images)
        }
    
    async def _call_vision_api(self, img_base64: str, page_num: int) -> str:
        """GPT-4o Vision API í˜¸ì¶œ"""
        response = await self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """ì´ ë³´í—˜ ì•½ê´€ í˜ì´ì§€ì˜ ëª¨ë“  ë‚´ìš©ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
                        
ê·œì¹™:
1. í‘œëŠ” Markdown í…Œì´ë¸”ë¡œ ë³€í™˜
2. ì´ë¯¸ì§€ëŠ” [ì´ë¯¸ì§€: ì„¤ëª…] í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
3. ì¡°í•­ ë²ˆí˜¸ì™€ ì œëª©ì„ ì •í™•íˆ ì¶”ì¶œ
4. í˜ì´ì§€ ë²ˆí˜¸ ì •ë³´ í¬í•¨
5. ì›ë³¸ ë ˆì´ì•„ì›ƒì„ ìµœëŒ€í•œ ìœ ì§€"""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{img_base64}",
                            "detail": "high"  # ê³ í•´ìƒë„ ë¶„ì„
                        }
                    }
                ]
            }],
            max_tokens=4096,
            temperature=0.1  # ì¼ê´€ì„± ìˆëŠ” ë³€í™˜
        )
        
        return response.choices[0].message.content
```

### 5.4 í•˜ì´ë¸Œë¦¬ë“œ ë³‘í•© ì „ëµ

```python
from difflib import SequenceMatcher

class HybridMerger:
    """Path 1ê³¼ Path 2ì˜ ê²°ê³¼ë¥¼ ë³‘í•©"""
    
    def merge(
        self,
        pymupdf_result: dict,
        vision_result: dict,
        threshold: float = 0.7
    ) -> dict:
        """
        ìœ ì‚¬ë„ ê¸°ë°˜ ë³‘í•©
        
        Args:
            pymupdf_result: PyMuPDF ê²°ê³¼
            vision_result: Vision ê²°ê³¼
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        
        Returns:
            ë³‘í•©ëœ ê²°ê³¼
        """
        # í˜ì´ì§€ë³„ë¡œ ë¶„ë¦¬
        pymupdf_pages = self._split_pages(pymupdf_result['markdown'])
        vision_pages = self._split_pages(vision_result['markdown'])
        
        merged_pages = []
        
        for idx, (p1, p2) in enumerate(zip(pymupdf_pages, vision_pages)):
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = SequenceMatcher(None, p1, p2).ratio()
            
            if similarity >= threshold:
                # ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ PyMuPDF ì„ íƒ (ë¹ ë¥´ê³  ì •í™•)
                merged_pages.append(p1)
                logger.debug(f"í˜ì´ì§€ {idx+1}: PyMuPDF ì„ íƒ (ìœ ì‚¬ë„={similarity:.2f})")
            else:
                # ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ Vision ì„ íƒ (í‘œ/ì´ë¯¸ì§€ ì²˜ë¦¬ ìš°ìˆ˜)
                merged_pages.append(p2)
                logger.debug(f"í˜ì´ì§€ {idx+1}: Vision ì„ íƒ (ìœ ì‚¬ë„={similarity:.2f})")
        
        # ìµœì¢… ë³‘í•©
        merged_markdown = "\n\n---\n\n".join(merged_pages)
        
        return {
            'markdown': merged_markdown,
            'quality_score': self._calculate_merged_quality(
                pymupdf_result, vision_result
            )
        }
```

### 5.5 ì²­í‚¹ ì „ëµ

```python
import tiktoken

class TextChunker:
    """í…ìŠ¤íŠ¸ë¥¼ ê³ ì • í¬ê¸° ì²­í¬ë¡œ ë¶„í• """
    
    def __init__(self, chunk_size: int = 1000, overlap: int = 100):
        """
        Args:
            chunk_size: ì²­í¬ í¬ê¸° (í† í°)
            overlap: ì¤‘ì²© í¬ê¸° (í† í°)
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.encoding = tiktoken.get_encoding("cl100k_base")
    
    def chunk(self, text: str, document_id: int) -> List[dict]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ [
                {
                    'content': str,
                    'chunk_index': int,
                    'token_count': int,
                    'page_number': int,
                    'section_title': str,
                    'clause_number': str,
                    'metadata': dict
                },
                ...
            ]
        """
        # í‘œ ë¶„ë¦¬ ì²˜ë¦¬
        tables = self._extract_tables(text)
        
        # í‘œë¥¼ ì œì™¸í•œ í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = []
        tokens = self.encoding.encode(text)
        
        start = 0
        chunk_idx = 0
        
        while start < len(tokens):
            end = start + self.chunk_size
            chunk_tokens = tokens[start:end]
            chunk_text = self.encoding.decode(chunk_tokens)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self._extract_metadata(chunk_text)
            
            chunks.append({
                'content': chunk_text,
                'chunk_index': chunk_idx,
                'token_count': len(chunk_tokens),
                'page_number': metadata.get('page_number'),
                'section_title': metadata.get('section_title'),
                'clause_number': metadata.get('clause_number'),
                'chunk_type': 'text',
                'metadata': metadata
            })
            
            chunk_idx += 1
            start = end - self.overlap  # ì˜¤ë²„ë© ì ìš©
        
        # í‘œë¥¼ ë³„ë„ ì²­í¬ë¡œ ì¶”ê°€
        for table in tables:
            chunks.append({
                'content': table['content'],
                'chunk_index': chunk_idx,
                'token_count': len(self.encoding.encode(table['content'])),
                'page_number': table.get('page_number'),
                'section_title': table.get('section_title'),
                'clause_number': None,
                'chunk_type': 'table',
                'metadata': table['metadata']
            })
            chunk_idx += 1
        
        return chunks
```

### 5.6 ì„ë² ë”© ìƒì„± ë° ìºì‹±

```python
from openai import AsyncOpenAI
import hashlib
import redis.asyncio as redis

class EmbeddingService:
    """ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤ (Redis ìºì‹±)"""
    
    def __init__(self):
        self.client = AsyncOpenAI()
        self.redis = redis.from_url("redis://localhost:6379/0")
        self.model = "text-embedding-3-large"
        self.dimensions = 1536
    
    async def generate_embeddings(
        self,
        texts: List[str],
        use_cache: bool = True
    ) -> List[List[float]]:
        """
        ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []
        
        embeddings = []
        cache_misses = []
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            for text in texts:
                cache_key = self._get_cache_key(text)
                cached = await self.redis.get(cache_key)
                
                if cached:
                    embeddings.append(json.loads(cached))
                    logger.debug(f"ìºì‹œ íˆíŠ¸: {cache_key[:16]}...")
                else:
                    cache_misses.append(text)
                    embeddings.append(None)  # í”Œë ˆì´ìŠ¤í™€ë”
        else:
            cache_misses = texts
            embeddings = [None] * len(texts)
        
        # ìºì‹œ ë¯¸ìŠ¤ë§Œ API í˜¸ì¶œ
        if cache_misses:
            logger.info(f"ì„ë² ë”© ìƒì„±: {len(cache_misses)}ê°œ (ìºì‹œ ë¯¸ìŠ¤)")
            
            response = await self.client.embeddings.create(
                model=self.model,
                input=cache_misses,
                dimensions=self.dimensions
            )
            
            # ê²°ê³¼ ì €ì¥ ë° ìºì‹±
            miss_idx = 0
            for i, emb in enumerate(embeddings):
                if emb is None:  # ìºì‹œ ë¯¸ìŠ¤ì˜€ë˜ í•­ëª©
                    embedding = response.data[miss_idx].embedding
                    embeddings[i] = embedding
                    
                    # Redisì— ìºì‹± (TTL: 30ì¼)
                    if use_cache:
                        cache_key = self._get_cache_key(cache_misses[miss_idx])
                        await self.redis.setex(
                            cache_key,
                            30 * 24 * 3600,  # 30ì¼
                            json.dumps(embedding)
                        )
                    
                    miss_idx += 1
        
        return embeddings
    
    def _get_cache_key(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ìºì‹œ í‚¤ ìƒì„± (SHA-256 í•´ì‹œ)"""
        return f"emb:{hashlib.sha256(text.encode()).hexdigest()}"
```

---

## 6. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ

### 6.1 ê²€ìƒ‰ ì „ëµ

```python
class HybridSearchService:
    """ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ ìœµí•©"""
    
    RRF_K = 60  # RRF íŒŒë¼ë¯¸í„°
    MAX_CONTEXT_TOKENS = 20000  # GPT-4o ì»¨í…ìŠ¤íŠ¸ ì œí•œ
    
    async def hybrid_search(
        self,
        session: AsyncSession,
        query: str,
        limit: int = 10,
        vector_weight: float = 0.7,
        keyword_weight: float = 0.3
    ) -> List[dict]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            limit: ê²°ê³¼ ê°œìˆ˜
            vector_weight: ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜
            keyword_weight: í‚¤ì›Œë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜
        
        Returns:
            ìœµí•©ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        # 1. ì§ˆì˜ ì „ì²˜ë¦¬
        preprocessed = self.preprocessor.preprocess(query)
        
        # 2. ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰
        vector_task = self.vector_search(
            session,
            preprocessed.expanded_query,
            limit=limit * 2
        )
        keyword_task = self.keyword_search(
            session,
            preprocessed.normalized_query,
            limit=limit * 2
        )
        
        vector_results, keyword_results = await asyncio.gather(
            vector_task,
            keyword_task
        )
        
        # 3. RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìœµí•©
        merged_results = self._rrf_merge(
            vector_results,
            keyword_results,
            vector_weight,
            keyword_weight
        )
        
        # 4. ì»¨í…ìŠ¤íŠ¸ ìµœì í™” (í† í° ì œí•œ)
        optimized_results = self._optimize_context(
            merged_results,
            max_tokens=self.MAX_CONTEXT_TOKENS
        )
        
        return optimized_results[:limit]
```

### 6.2 RRF (Reciprocal Rank Fusion) ì•Œê³ ë¦¬ì¦˜

```python
def _rrf_merge(
    self,
    vector_results: List[dict],
    keyword_results: List[dict],
    vector_weight: float = 0.7,
    keyword_weight: float = 0.3
) -> List[dict]:
    """
    Reciprocal Rank Fusionìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ìœµí•©
    
    RRF ì ìˆ˜ = Î£ (weight / (k + rank))
    
    Args:
        vector_results: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        keyword_results: í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        vector_weight: ë²¡í„° ê°€ì¤‘ì¹˜
        keyword_weight: í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
    
    Returns:
        ìœµí•©ëœ ê²°ê³¼ (RRF ì ìˆ˜ ê¸°ì¤€ ì •ë ¬)
    """
    scores = {}
    
    # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜
    for rank, result in enumerate(vector_results, start=1):
        chunk_id = result['id']
        rrf_score = vector_weight / (self.RRF_K + rank)
        
        if chunk_id not in scores:
            scores[chunk_id] = {
                'chunk_id': chunk_id,
                'data': result,
                'rrf_score': 0,
                'vector_rank': rank,
                'keyword_rank': None
            }
        
        scores[chunk_id]['rrf_score'] += rrf_score
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜
    for rank, result in enumerate(keyword_results, start=1):
        chunk_id = result['id']
        rrf_score = keyword_weight / (self.RRF_K + rank)
        
        if chunk_id not in scores:
            scores[chunk_id] = {
                'chunk_id': chunk_id,
                'data': result,
                'rrf_score': 0,
                'vector_rank': None,
                'keyword_rank': rank
            }
        else:
            scores[chunk_id]['keyword_rank'] = rank
        
        scores[chunk_id]['rrf_score'] += rrf_score
    
    # RRF ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    sorted_results = sorted(
        scores.values(),
        key=lambda x: x['rrf_score'],
        reverse=True
    )
    
    # ë°ì´í„° ì¶”ì¶œ
    return [item['data'] for item in sorted_results]
```

### 6.3 ì»¨í…ìŠ¤íŠ¸ ìµœì í™”

```python
def _optimize_context(
    self,
    search_results: List[dict],
    max_tokens: int = 20000
) -> List[dict]:
    """
    í† í° ì œí•œ ë‚´ì—ì„œ ìµœì ì˜ ì»¨í…ìŠ¤íŠ¸ ì„ íƒ
    
    Args:
        search_results: ê²€ìƒ‰ ê²°ê³¼
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
    
    Returns:
        ìµœì í™”ëœ ê²€ìƒ‰ ê²°ê³¼
    """
    optimized = []
    total_tokens = 0
    
    for result in search_results:
        # í† í° ì¹´ìš´íŠ¸
        chunk_tokens = len(self.encoding.encode(result['content']))
        
        if total_tokens + chunk_tokens > max_tokens:
            logger.debug(
                f"ì»¨í…ìŠ¤íŠ¸ ìµœì í™”: {len(optimized)}ê°œ ì²­í¬, "
                f"{total_tokens} í† í° (ì œí•œ: {max_tokens})"
            )
            break
        
        optimized.append(result)
        total_tokens += chunk_tokens
    
    return optimized
```

---

## 7. ë‹µë³€ ê²€ì¦ ì‹œìŠ¤í…œ

### 7.1 4ë‹¨ê³„ ê²€ì¦ í”„ë¡œì„¸ìŠ¤

```python
class AnswerValidator:
    """ë‹µë³€ ê²€ì¦ ì„œë¹„ìŠ¤"""
    
    # ê²€ì¦ ê°€ì¤‘ì¹˜
    WEIGHTS = {
        "hallucination": 0.4,  # í• ë£¨ì‹œë„¤ì´ì…˜ 40%
        "context": 0.3,        # ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ 30%
        "clause": 0.2,         # ì¡°í•­ ì¡´ì¬ 20%
        "format": 0.1          # í˜•ì‹ 10%
    }
    
    async def validate(
        self,
        answer: str,
        query: str,
        search_results: List[dict],
        session: AsyncSession
    ) -> AnswerValidation:
        """
        ë‹µë³€ ê²€ì¦ (4ë‹¨ê³„)
        
        Returns:
            AnswerValidation ê°ì²´
        """
        # 1. í˜•ì‹ ê²€ì¦
        format_validation = self._check_format(answer, search_results)
        
        # 2. ì¡°í•­ ì¡´ì¬ í™•ì¸
        clause_validation = await self._check_clause_existence(
            answer, session
        )
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ì¼ì¹˜ë„
        context_validation = self._check_context_consistency(
            answer, search_results
        )
        
        # 4. í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ (GPT-4o)
        hallucination_validation = await self._check_hallucination(
            answer, query, search_results
        )
        
        # ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(
            format_validation,
            clause_validation,
            context_validation,
            hallucination_validation
        )
        
        return AnswerValidation(
            confidence_score=confidence,
            is_valid=confidence >= 0.7,
            format_check=format_validation,
            clause_check=clause_validation,
            context_check=context_validation,
            hallucination_check=hallucination_validation
        )
```

### 7.2 í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦

```python
async def _check_hallucination(
    self,
    answer: str,
    query: str,
    search_results: List[dict]
) -> ValidationDetail:
    """
    GPT-4oë¥¼ ì‚¬ìš©í•œ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦
    
    ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê¸°ë°˜í–ˆëŠ”ì§€ í™•ì¸
    """
    context = "\n\n".join([
        f"[ì²­í¬ {i+1}]\n{r['content']}"
        for i, r in enumerate(search_results)
    ])
    
    prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ì•½ê´€ ë‹µë³€ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:
{context}

ìƒì„±ëœ ë‹µë³€:
{answer}

ê²€ì¦ ê¸°ì¤€:
1. ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ëŠ”ê°€?
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šì•˜ëŠ”ê°€?
3. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ìƒì‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ëŠ”ê°€?

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "is_grounded": true/false,
    "hallucinated_statements": ["ë¬¸ì¥1", "ë¬¸ì¥2", ...],
    "confidence": 0.0~1.0,
    "reason": "ê²€ì¦ ì´ìœ "
}}"""

    response = await self.client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
        response_format={"type": "json_object"}
    )
    
    result = json.loads(response.choices[0].message.content)
    
    return ValidationDetail(
        passed=result['is_grounded'],
        score=result['confidence'],
        details={
            'hallucinated_statements': result['hallucinated_statements'],
            'reason': result['reason']
        }
    )
```

### 7.3 ì¡°í•­ ì¡´ì¬ í™•ì¸

```python
async def _check_clause_existence(
    self,
    answer: str,
    session: AsyncSession
) -> ValidationDetail:
    """
    ë‹µë³€ì— ì–¸ê¸‰ëœ ì¡°í•­ ë²ˆí˜¸ê°€ ì‹¤ì œë¡œ DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    """
    # ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ
    clause_pattern = r'ì œ\s*(\d+)\s*ì¡°'
    clauses = re.findall(clause_pattern, answer)
    
    if not clauses:
        return ValidationDetail(
            passed=True,
            score=1.0,
            details={'message': 'ì¡°í•­ ë²ˆí˜¸ ì—†ìŒ'}
        )
    
    # DBì—ì„œ ì¡°í•­ ì¡´ì¬ í™•ì¸
    existing_clauses = []
    missing_clauses = []
    
    for clause_num in clauses:
        query = text("""
            SELECT COUNT(*) as count
            FROM document_chunks
            WHERE clause_number LIKE :pattern
        """)
        
        result = await session.execute(
            query,
            {'pattern': f'%ì œ{clause_num}ì¡°%'}
        )
        
        count = result.scalar()
        
        if count > 0:
            existing_clauses.append(clause_num)
        else:
            missing_clauses.append(clause_num)
    
    passed = len(missing_clauses) == 0
    score = len(existing_clauses) / len(clauses) if clauses else 1.0
    
    return ValidationDetail(
        passed=passed,
        score=score,
        details={
            'total_clauses': len(clauses),
            'existing_clauses': existing_clauses,
            'missing_clauses': missing_clauses
        }
    )
```

---

## 8. ì§ˆì˜ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ

### 8.1 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
class QueryPreprocessor:
    """ì‚¬ìš©ì ì§ˆì˜ ì „ì²˜ë¦¬"""
    
    def preprocess(self, query: str) -> PreprocessedQuery:
        """
        ì§ˆì˜ ì „ì²˜ë¦¬
        
        Returns:
            PreprocessedQuery(
                original_query: str,
                normalized_query: str,
                expanded_query: str,
                keywords: List[str],
                clause_numbers: List[str],
                is_incomplete: bool,
                suggestions: List[str]
            )
        """
        # 1. ì •ê·œí™”
        normalized = self._normalize(query)
        
        # 2. ì „ë¬¸ìš©ì–´ í‘œì¤€í™”
        standardized = self._standardize_terms(normalized)
        
        # 3. ë™ì˜ì–´ í™•ì¥
        expanded = self._expand_synonyms(standardized)
        
        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(expanded)
        
        # 5. ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ
        clause_numbers = self._extract_clause_numbers(normalized)
        
        # 6. ë¶ˆì™„ì „ ì§ˆì˜ ê°ì§€
        is_incomplete, suggestions = self._detect_incomplete(normalized)
        
        return PreprocessedQuery(
            original_query=query,
            normalized_query=normalized,
            expanded_query=expanded,
            keywords=keywords,
            clause_numbers=clause_numbers,
            is_incomplete=is_incomplete,
            suggestions=suggestions
        )
```

### 8.2 ì „ë¬¸ìš©ì–´ í‘œì¤€í™”

```python
def _standardize_terms(self, query: str) -> str:
    """
    ë³´í—˜ ì „ë¬¸ìš©ì–´ í‘œì¤€í™”
    
    ì˜ˆ: "ì•”ë³´í—˜" â†’ "ì•” ë³´í—˜"
         "ë³´í—˜ë£Œ" â†’ "ë³´í—˜ë£Œ"
         "CI" â†’ "ì¤‘ëŒ€í•œ ì§ˆë³‘"
    """
    standardized = query
    
    # ë„ì–´ï¿½ê¸° ê·œì¹™ ì ìš©
    for wrong, correct in self.spacing_rules.items():
        standardized = standardized.replace(wrong, correct)
    
    # ì•½ì–´ í™•ì¥
    for abbr, full in self.abbreviations.items():
        # ë‹¨ì–´ ê²½ê³„ì—ì„œë§Œ ì¹˜í™˜
        pattern = r'\b' + re.escape(abbr) + r'\b'
        standardized = re.sub(pattern, full, standardized, flags=re.IGNORECASE)
    
    return standardized
```

### 8.3 ë™ì˜ì–´ í™•ì¥

```python
def _expand_synonyms(self, query: str) -> str:
    """
    ë™ì˜ì–´ í™•ì¥
    
    ì˜ˆ: "ë³´í—˜ê¸ˆ" â†’ "ë³´í—˜ê¸ˆ ë˜ëŠ” ê¸‰ì—¬ê¸ˆ ë˜ëŠ” ì§€ê¸‰ê¸ˆ"
    """
    expanded_terms = []
    
    words = query.split()
    
    for word in words:
        # ë™ì˜ì–´ ê·¸ë£¹ ì°¾ê¸°
        synonyms = self._find_synonyms(word)
        
        if synonyms:
            # "word ë˜ëŠ” synonym1 ë˜ëŠ” synonym2" í˜•ì‹
            expanded = f"{word} {' '.join(['ë˜ëŠ” ' + s for s in synonyms])}"
            expanded_terms.append(expanded)
        else:
            expanded_terms.append(word)
    
    return ' '.join(expanded_terms)
```

### 8.4 ì „ë¬¸ìš©ì–´ ì‚¬ì „ (insurance_terms.json)

```json
{
  "normalization": {
    "spacing": {
      "ì•”ë³´í—˜": "ì•” ë³´í—˜",
      "ê±´ê°•ë³´í—˜": "ê±´ê°• ë³´í—˜",
      "ë³´í—˜ë£Œ": "ë³´í—˜ë£Œ"
    }
  },
  "synonyms": {
    "ë³´í—˜ê¸ˆ": ["ê¸‰ì—¬ê¸ˆ", "ì§€ê¸‰ê¸ˆ", "ë³´ìƒê¸ˆ"],
    "ê³„ì•½ì": ["ê°€ì…ì", "ê³„ì•½ ë‹¹ì‚¬ì"],
    "í”¼ë³´í—˜ì": ["ë³´í—˜ ëŒ€ìƒì", "ë³´í—˜ ê°€ì…ì"],
    "ìˆ˜ìµì": ["ë³´í—˜ê¸ˆ ìˆ˜ë ¹ì¸", "ìˆ˜ì·¨ì¸"]
  },
  "abbreviations": {
    "CI": "ì¤‘ëŒ€í•œ ì§ˆë³‘",
    "TCM": "ì „í†µ ì˜í•™",
    "MRI": "ìê¸°ê³µëª…ì˜ìƒ"
  },
  "incomplete_patterns": [
    {
      "pattern": "^(ë³´ì¥|ë³´í—˜ê¸ˆ|ì²­êµ¬|í•´ì§€|ë©´ì±…)$",
      "suggestion": "êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. ì˜ˆ: 'ë³´ì¥ ë²”ìœ„ëŠ”?', 'ë³´í—˜ê¸ˆì€ ì–¸ì œ?'"
    },
    {
      "pattern": "^\\w{1,3}$",
      "suggestion": "ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì„¸ìš”."
    }
  ]
}
```

---

## 9. ì²­í¬ í™•ì¥ ë©”ì»¤ë‹ˆì¦˜

### 9.1 í™•ì¥ ì „ëµ

```python
class ChunkExpansionService:
    """ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ì‹œ ì²­í¬ í™•ì¥"""
    
    MAX_EXPANSION_COUNT = 3  # ìµœëŒ€ í™•ì¥ íšŸìˆ˜
    
    async def expand_chunks(
        self,
        chunk_ids: List[int],
        session: AsyncSession,
        expansion_count: int
    ) -> List[dict]:
        """
        ì²­í¬ í™•ì¥
        
        ì „ëµ:
        1. ê°™ì€ ë¬¸ì„œì˜ ì•ë’¤ ì²­í¬ (Â±2 ì¸ë±ìŠ¤)
        2. ê°™ì€ í˜ì´ì§€ì˜ ì²­í¬
        3. ê°™ì€ ì„¹ì…˜ì˜ ì²­í¬
        
        Args:
            chunk_ids: í™•ì¥í•  ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
            session: DB ì„¸ì…˜
            expansion_count: í˜„ì¬ í™•ì¥ íšŸìˆ˜
        
        Returns:
            í™•ì¥ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if expansion_count >= self.MAX_EXPANSION_COUNT:
            logger.warning(f"ìµœëŒ€ í™•ì¥ íšŸìˆ˜ ì´ˆê³¼: {expansion_count}")
            return []
        
        expanded_chunks = []
        
        for chunk_id in chunk_ids:
            # ì²­í¬ ì •ë³´ ì¡°íšŒ
            chunk = await self._get_chunk(session, chunk_id)
            if not chunk:
                continue
            
            # ì „ëµ 1: ì•ë’¤ ì²­í¬
            neighbor_chunks = await self._get_neighbor_chunks(
                session,
                chunk['document_id'],
                chunk['chunk_index'],
                window=2
            )
            expanded_chunks.extend(neighbor_chunks)
            
            # ì „ëµ 2: ê°™ì€ í˜ì´ì§€
            page_chunks = await self._get_page_chunks(
                session,
                chunk['document_id'],
                chunk['page_number']
            )
            expanded_chunks.extend(page_chunks)
            
            # ì „ëµ 3: ê°™ì€ ì„¹ì…˜
            if chunk.get('section_title'):
                section_chunks = await self._get_section_chunks(
                    session,
                    chunk['document_id'],
                    chunk['section_title']
                )
                expanded_chunks.extend(section_chunks)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_chunks = self._deduplicate(expanded_chunks)
        
        logger.info(
            f"ì²­í¬ í™•ì¥ ì™„ë£Œ: {len(chunk_ids)}ê°œ â†’ {len(unique_chunks)}ê°œ "
            f"(í™•ì¥ íšŸìˆ˜: {expansion_count + 1})"
        )
        
        return unique_chunks
```

### 9.2 ì•ë’¤ ì²­í¬ í™•ì¥

```python
async def _get_neighbor_chunks(
    self,
    session: AsyncSession,
    document_id: int,
    chunk_index: int,
    window: int = 2
) -> List[dict]:
    """
    ì•ë’¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    
    ì˜ˆ: chunk_index=10, window=2
    â†’ chunk_index in [8, 9, 10, 11, 12]
    """
    query = text("""
        SELECT 
            id,
            document_id,
            chunk_index,
            content,
            page_number,
            section_title,
            clause_number
        FROM document_chunks
        WHERE document_id = :document_id
            AND chunk_index BETWEEN :start_idx AND :end_idx
        ORDER BY chunk_index
    """)
    
    result = await session.execute(query, {
        'document_id': document_id,
        'start_idx': chunk_index - window,
        'end_idx': chunk_index + window
    })
    
    return [dict(row._mapping) for row in result.fetchall()]
```

---

## 10. ëŒ€í™” ì´ë ¥ ê´€ë¦¬

### 10.1 ì„¸ì…˜ ê¸°ë°˜ ì´ë ¥ ê´€ë¦¬

```python
class ChatHistoryService:
    """ëŒ€í™” ì´ë ¥ ê´€ë¦¬ ì„œë¹„ìŠ¤"""
    
    async def create_session(
        self,
        session: AsyncSession,
        user_id: Optional[int] = None
    ) -> ChatSession:
        """ìƒˆ ëŒ€í™” ì„¸ì…˜ ìƒì„±"""
        chat_session = ChatSession(
            user_id=user_id,
            session_id=str(uuid.uuid4()),
            created_at=datetime.utcnow()
        )
        
        session.add(chat_session)
        await session.commit()
        await session.refresh(chat_session)
        
        return chat_session
    
    async def add_message(
        self,
        session: AsyncSession,
        session_id: str,
        role: str,
        content: str,
        metadata: Optional[dict] = None
    ) -> ChatMessage:
        """ë©”ì‹œì§€ ì¶”ê°€"""
        message = ChatMessage(
            session_id=session_id,
            role=role,  # 'user' or 'assistant'
            content=content,
            metadata=metadata,
            created_at=datetime.utcnow()
        )
        
        session.add(message)
        await session.commit()
        await session.refresh(message)
        
        return message
    
    async def get_session_history(
        self,
        session: AsyncSession,
        session_id: str,
        limit: int = 50
    ) -> List[ChatMessage]:
        """ì„¸ì…˜ì˜ ëŒ€í™” ì´ë ¥ ì¡°íšŒ"""
        query = text("""
            SELECT *
            FROM chat_messages
            WHERE session_id = :session_id
            ORDER BY created_at ASC
            LIMIT :limit
        """)
        
        result = await session.execute(query, {
            'session_id': session_id,
            'limit': limit
        })
        
        return [ChatMessage(**row._mapping) for row in result.fetchall()]
```

---

## 11. API ì—”ë“œí¬ì¸íŠ¸

### 11.1 FastAPI ë¼ìš°í„° êµ¬ì¡°

```
backend/api/
  â”œâ”€â”€ health.py         # í—¬ìŠ¤ ì²´í¬
  â”œâ”€â”€ chat.py           # ì±—ë´‡ ëŒ€í™”
  â”œâ”€â”€ chat_history.py   # ëŒ€í™” ì´ë ¥
  â”œâ”€â”€ search.py         # ê²€ìƒ‰
  â”œâ”€â”€ documents.py      # ì•½ê´€ ê´€ë¦¬
  â””â”€â”€ pdf.py            # PDF ì¡°íšŒ
```

### 11.2 ì£¼ìš” API

```python
# 1. ì±—ë´‡ ëŒ€í™”
POST /api/chat
Request:
{
  "query": "ê³¨ì ˆ ì‹œ ë³´ì¥ ì—¬ë¶€ëŠ”?",
  "session_id": "uuid",
  "stream": false
}
Response:
{
  "answer": "ğŸ“Œ ë‹µë³€\n...\nğŸ“‹ ê´€ë ¨ ì•½ê´€\n...",
  "search_results": [...],
  "validation": {
    "confidence_score": 0.85,
    "is_valid": true
  },
  "processing_time_ms": 1234
}

# 2. ìŠ¤íŠ¸ë¦¬ë° ëŒ€í™”
POST /api/chat/stream
Response: text/event-stream

# 3. ëŒ€í™” ì´ë ¥ ì¡°íšŒ
GET /api/chat/history/{session_id}
Response:
{
  "session_id": "uuid",
  "messages": [
    {"role": "user", "content": "ì§ˆë¬¸", "created_at": "..."},
    {"role": "assistant", "content": "ë‹µë³€", "created_at": "..."}
  ]
}

# 4. ì•½ê´€ ì—…ë¡œë“œ
POST /api/documents/upload
Request: multipart/form-data
  file: PDF íŒŒì¼
  document_type: "policy"
  insurance_type: "life"
Response:
{
  "document_id": 123,
  "filename": "ìƒí’ˆëª….pdf",
  "processing_status": "completed",
  "quality_score": 0.92,
  "chunks_count": 456
}

# 5. ì•½ê´€ ëª©ë¡
GET /api/documents?page=1&limit=20
Response:
{
  "documents": [
    {
      "id": 123,
      "filename": "ìƒí’ˆëª….pdf",
      "document_type": "policy",
      "status": "active",
      "upload_timestamp": "...",
      "total_pages": 50
    }
  ],
  "total": 100,
  "page": 1,
  "pages": 5
}

# 6. ì•½ê´€ ì‚­ì œ
DELETE /api/documents/{document_id}

# 7. PDF ì¡°íšŒ
GET /api/pdf/{document_id}
Response: application/pdf

# 8. ê²€ìƒ‰
POST /api/search
Request:
{
  "query": "ê³¨ì ˆ ë³´ì¥",
  "limit": 10,
  "document_ids": [123, 456]
}
Response:
{
  "results": [
    {
      "chunk_id": 789,
      "content": "...",
      "similarity": 0.85,
      "document_name": "ìƒí’ˆëª….pdf",
      "page_number": 10
    }
  ]
}
```

---

## 12. Frontend êµ¬ì¡°

### 12.1 Next.js App Router êµ¬ì¡°

```
frontend/
  â”œâ”€â”€ app/
  â”‚   â”œâ”€â”€ layout.tsx           # ë£¨íŠ¸ ë ˆì´ì•„ì›ƒ
  â”‚   â”œâ”€â”€ page.tsx             # í™ˆ (ëŒ€ì‹œë³´ë“œ)
  â”‚   â”œâ”€â”€ chat/
  â”‚   â”‚   â””â”€â”€ page.tsx         # ì±—ë´‡ ëŒ€í™”
  â”‚   â”œâ”€â”€ documents/
  â”‚   â”‚   â””â”€â”€ page.tsx         # ì•½ê´€ ê´€ë¦¬
  â”‚   â””â”€â”€ history/
  â”‚       â””â”€â”€ page.tsx         # ëŒ€í™” ì´ë ¥
  â”œâ”€â”€ components/
  â”‚   â”œâ”€â”€ AppLayout.tsx        # ë©”ì¸ ë ˆì´ì•„ì›ƒ
  â”‚   â”œâ”€â”€ Sidebar.tsx          # ì‚¬ì´ë“œë°”
  â”‚   â”œâ”€â”€ ChatMessage.tsx      # ì±— ë©”ì‹œì§€
  â”‚   â”œâ”€â”€ ChatInput.tsx        # ì±— ì…ë ¥
  â”‚   â”œâ”€â”€ ChatHistory.tsx      # ëŒ€í™” ì´ë ¥
  â”‚   â”œâ”€â”€ DocumentUpload.tsx   # ì•½ê´€ ì—…ë¡œë“œ
  â”‚   â”œâ”€â”€ DocumentViewer.tsx   # ì•½ê´€ ì¡°íšŒ
  â”‚   â”œâ”€â”€ ReferencePanel.tsx   # ì°¸ì¡° íŒ¨ë„
  â”‚   â””â”€â”€ DeleteConfirmDialog.tsx
  â””â”€â”€ lib/
      â””â”€â”€ api.ts               # API í´ë¼ì´ì–¸íŠ¸
```

### 12.2 ì£¼ìš” ì»´í¬ë„ŒíŠ¸

```typescript
// ChatMessage.tsx - ì±— ë©”ì‹œì§€ ë Œë”ë§
export default function ChatMessage({ 
  message 
}: { 
  message: Message 
}) {
  return (
    <div className={`message ${message.role}`}>
      {message.role === 'assistant' ? (
        <ReactMarkdown remarkPlugins={[remarkGfm]}>
          {message.content}
        </ReactMarkdown>
      ) : (
        <p>{message.content}</p>
      )}
    </div>
  );
}

// ChatInput.tsx - ì±— ì…ë ¥
export default function ChatInput({ 
  onSend 
}: { 
  onSend: (query: string) => void 
}) {
  const [query, setQuery] = useState('');
  
  const handleSubmit = () => {
    if (!query.trim()) return;
    onSend(query);
    setQuery('');
  };
  
  return (
    <div className="chat-input">
      <textarea
        value={query}
        onChange={(e) => setQuery(e.target.value)}
        placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."
        onKeyDown={(e) => {
          if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            handleSubmit();
          }
        }}
      />
      <button onClick={handleSubmit}>ì „ì†¡</button>
    </div>
  );
}

// ReferencePanel.tsx - ì°¸ì¡° íŒ¨ë„
export default function ReferencePanel({ 
  references 
}: { 
  references: Reference[] 
}) {
  return (
    <div className="reference-panel">
      <h3>ğŸ“‹ ì°¸ì¡° ë¬¸ì„œ</h3>
      {references.map((ref, idx) => (
        <div key={idx} className="reference-item">
          <div className="ref-header">
            [ì°¸ì¡° {idx + 1}] {ref.filename}
          </div>
          <div className="ref-meta">
            í˜ì´ì§€: {ref.page_number} | 
            ìœ ì‚¬ë„: {(ref.similarity * 100).toFixed(1)}%
          </div>
          <div className="ref-content">
            {ref.content.substring(0, 200)}...
          </div>
        </div>
      ))}
    </div>
  );
}
```

---

## 13. ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 13.1 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

```sql
-- 1. HNSW ì¸ë±ìŠ¤ íŒŒë¼ë¯¸í„° íŠœë‹
CREATE INDEX idx_chunks_embedding ON document_chunks 
USING hnsw (embedding vector_cosine_ops) 
WITH (
    m = 32,              -- 32ë¡œ ì¦ê°€ (ê¸°ë³¸ê°’: 16)
    ef_construction = 200 -- 200ìœ¼ë¡œ ì¦ê°€ (ê¸°ë³¸ê°’: 64)
);

-- 2. ì—°ê²° í’€ ì„¤ì •
-- SQLAlchemy Engine:
--   pool_size = 10
--   max_overflow = 20
--   pool_timeout = 30

-- 3. ì¿¼ë¦¬ ìµœì í™”
-- ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€
CREATE INDEX idx_chunks_document_page 
ON document_chunks(document_id, page_number);

CREATE INDEX idx_chunks_document_clause 
ON document_chunks(document_id, clause_number);

-- 4. VACUUM ANALYZE (ì •ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)
VACUUM ANALYZE document_chunks;
```

### 13.2 Redis ìºì‹± ì „ëµ

```python
# ì„ë² ë”© ìºì‹±
CACHE_KEY_PREFIX = "emb:"
CACHE_TTL = 30 * 24 * 3600  # 30ì¼

# ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§
async def get_cache_stats(redis_client):
    info = await redis_client.info("stats")
    hits = info['keyspace_hits']
    misses = info['keyspace_misses']
    hit_rate = hits / (hits + misses) if (hits + misses) > 0 else 0
    
    logger.info(f"ìºì‹œ íˆíŠ¸ìœ¨: {hit_rate:.2%} (íˆíŠ¸: {hits}, ë¯¸ìŠ¤: {misses})")
    
    return hit_rate
```

### 13.3 ë°°ì¹˜ ì²˜ë¦¬

```python
# ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
BATCH_SIZE = 100

async def batch_embed(texts: List[str]) -> List[List[float]]:
    embeddings = []
    
    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i:i + BATCH_SIZE]
        batch_embeddings = await embedding_service.generate_embeddings(batch)
        embeddings.extend(batch_embeddings)
    
    return embeddings
```

### 13.4 ë¹„ë™ê¸° ì²˜ë¦¬

```python
# PDF ì²˜ë¦¬ ë¹„ë™ê¸°í™”
async def process_pdf_async(pdf_path: str, document_id: int):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ PDF ì²˜ë¦¬"""
    # Path 1ê³¼ Path 2 ë³‘ë ¬ ì‹¤í–‰
    pymupdf_task = asyncio.create_task(
        pymupdf_extractor.extract_async(pdf_path)
    )
    vision_task = asyncio.create_task(
        vision_extractor.extract_async(pdf_path)
    )
    
    pymupdf_result, vision_result = await asyncio.gather(
        pymupdf_task,
        vision_task
    )
    
    # ë‚˜ë¨¸ì§€ ì²˜ë¦¬...
```

---

## 14. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 14.1 ë¡œê¹… ë ˆë²¨

```python
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/ispl.log'),
        logging.StreamHandler()
    ]
)

# ë ˆë²¨ë³„ ì‚¬ìš©
logger.debug("ë””ë²„ê¹… ì •ë³´")       # ê°œë°œ ì¤‘
logger.info("ì •ë³´ ë©”ì‹œì§€")        # ì •ìƒ íë¦„
logger.warning("ê²½ê³  ë©”ì‹œì§€")     # ë³µêµ¬ ê°€ëŠ¥í•œ ë¬¸ì œ
logger.error("ì˜¤ë¥˜ ë©”ì‹œì§€")       # ì²˜ë¦¬ ì‹¤íŒ¨
logger.critical("ì‹¬ê°í•œ ì˜¤ë¥˜")    # ì‹œìŠ¤í…œ ì¥ì• 
```

### 14.2 ì„±ëŠ¥ ë©”íŠ¸ë¦­

```python
# ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
import time

start_time = time.time()
# ... ì‘ì—… ìˆ˜í–‰ ...
processing_time = (time.time() - start_time) * 1000  # ms

logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ms")

# í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
logger.info(
    f"ì„ë² ë”© ìƒì„±: {len(texts)}ê°œ í…ìŠ¤íŠ¸, "
    f"ì´ {total_tokens} í† í°, "
    f"ë¹„ìš©: ${estimated_cost:.4f}"
)
```

---

## 15. ê¸°ìˆ ì  ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘ ë°©ì•ˆ

| ìœ„í—˜ ìš”ì†Œ | ì˜í–¥ë„ | ë°œìƒ ê°€ëŠ¥ì„± | ëŒ€ì‘ ë°©ì•ˆ | í˜„ì¬ ìƒíƒœ |
|---------|-------|-----------|---------|---------|
| PDF ì „ì²˜ë¦¬ í’ˆì§ˆ ì €í•˜ | ë†’ìŒ | ì¤‘ê°„ | í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹, í’ˆì§ˆ ê²€ì¦ ë‹¨ê³„ ì¶”ê°€ | âœ… êµ¬í˜„ ì™„ë£Œ |
| LLM í• ë£¨ì‹œë„¤ì´ì…˜ | ë†’ìŒ | ë†’ìŒ | 4ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ, ì—„ê²©í•œ í”„ë¡¬í”„íŠ¸ | âœ… êµ¬í˜„ ì™„ë£Œ |
| ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ ì €í•˜ | ì¤‘ê°„ | ì¤‘ê°„ | HNSW ì¸ë±ìŠ¤ ìµœì í™”, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ | âœ… êµ¬í˜„ ì™„ë£Œ |
| OpenAI API ë¹„ìš© | ì¤‘ê°„ | ë†’ìŒ | Redis ìºì‹±, ë°°ì¹˜ ì²˜ë¦¬, ëª¨ë‹ˆí„°ë§ | âœ… êµ¬í˜„ ì™„ë£Œ |
| Multi-Agent ë³µì¡ë„ | ì¤‘ê°„ | ì¤‘ê°„ | LangGraph í‘œì¤€ íŒ¨í„´, ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ | âœ… êµ¬í˜„ ì™„ë£Œ |
| ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± | ì¤‘ê°„ | ì¤‘ê°„ | ì²­í¬ í™•ì¥ ë©”ì»¤ë‹ˆì¦˜ (ìµœëŒ€ 3íšŒ) | âœ… êµ¬í˜„ ì™„ë£Œ |
| ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ | ë‚®ìŒ | ë‚®ìŒ | ì—°ê²° í’€ë§, ì¸ë±ìŠ¤ ìµœì í™” | âœ… êµ¬í˜„ ì™„ë£Œ |

---

## 16. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ (êµ¬í˜„ ë²„ì „)

### Backend (requirements.txt)

```
# FastAPI ë° ì›¹ í”„ë ˆì„ì›Œí¬
fastapi==0.115.0
uvicorn[standard]==0.32.0
python-multipart==0.0.12

# ë°ì´í„°ë² ì´ìŠ¤
sqlalchemy==2.0.35
asyncpg==0.30.0
psycopg2-binary==2.9.10
pgvector==0.3.6

# Redis (ìºì‹±)
redis[hiredis]==5.2.1

# PDF ì²˜ë¦¬
PyMuPDF==1.24.14
pymupdf4llm==0.0.17
pdf2image==1.17.0
Pillow==11.0.0

# ì´ë¯¸ì§€ ì²˜ë¦¬
opencv-python==4.10.0.84
numpy>=1.26.0,<2.0.0

# OpenAI ë° LangChain
openai==1.55.3
tiktoken==0.8.0
tenacity==9.0.0

# LangChain íŒ¨í‚¤ì§€
langchain==0.3.7
langchain-openai==0.2.8
langchain-community==0.3.5

# LangGraph
langgraph>=0.3.27

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.1
pydantic==2.10.2
pydantic-settings==2.6.1

# í•œê¸€ ì²˜ë¦¬
kiwipiepy=